{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score \n",
    "from glob import glob   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the grading information\n",
    "data = {\n",
    "    \"Letter Grade\": [\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"C+\", \"C\", \"U\"],\n",
    "    \"4-Point Equivalence\": [4.30, 4.00, 3.67, 3.33, 3.00, 2.33, 2.00, 0.00],\n",
    "    \"Grading Range\": [\"97 - 100\", \"94 - 96.99\", \"90 - 93.99\", \"87 - 89.99\", \"80 - 86.99\", \"77 - 79.99\", \"70 - 76.99\", \"<70\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export the dataframe to a CSV file\n",
    "csv_file_path = \"grading_scale.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "csv_file_path\n",
    "\n",
    "def load_grading_scale(file_path):\n",
    "    grading_df = pd.read_csv(file_path)\n",
    "    grade_map = dict(zip(grading_df['Letter Grade'], grading_df['4-Point Equivalence']))\n",
    "    return grade_map\n",
    "\n",
    "def convert_grade(value, grade_map, column_name):\n",
    "    columns_to_ignore = ['section', 'team', 'unique_id']\n",
    "    if column_name.lower() in columns_to_ignore:\n",
    "        return value\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        return grade_map.get(value.upper(), value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subdirectory: C100\n",
      "Processing file: data\\C100\\C100_SEC02.csv\n",
      "Processing file: data\\C100\\C100_SEC04.csv\n",
      "Processing file: data\\C100\\C100_SEC05.csv\n",
      "Processing file: data\\C100\\C100_SEC06.csv\n",
      "Processing file: data\\C100\\C100_SEC07.csv\n",
      "Processing file: data\\C100\\C100_SEC08.csv\n",
      "Processing file: data\\C100\\C100_SEC09.csv\n",
      "Processing file: data\\C100\\C100_SEC10.csv\n",
      "Processing file: data\\C100\\C100_SEC11.csv\n",
      "Processing file: data\\C100\\C100_SEC12.csv\n",
      "Processing file: data\\C100\\C100_SEC13.csv\n",
      "Processing file: data\\C100\\C100_SEC14.csv\n",
      "Processing file: data\\C100\\C100_SEC15.csv\n",
      "Processing file: data\\C100\\C100_SEC16.csv\n",
      "Processing file: data\\C100\\C100_SEC17.csv\n",
      "Processing file: data\\C100\\C100_SEC18.csv\n",
      "Processing file: data\\C100\\C100_SEC19.csv\n",
      "Removed columns: Username, learner_id, last_access, base_id\n",
      "Combined and processed data for C100 saved to: combined_data\\C100_combined.csv\n",
      "Processing subdirectory: C200\n",
      "Processing file: data\\C200\\C200_SEC02.csv\n",
      "Processing file: data\\C200\\C200_SEC04.csv\n",
      "Processing file: data\\C200\\C200_SEC05.csv\n",
      "Processing file: data\\C200\\C200_SEC06.csv\n",
      "Processing file: data\\C200\\C200_SEC07.csv\n",
      "Processing file: data\\C200\\C200_SEC08.csv\n",
      "Processing file: data\\C200\\C200_SEC09.csv\n",
      "Warning: Column count mismatch in data\\C200\\C200_SEC09.csv. Adjusting...\n",
      "Processing file: data\\C200\\C200_SEC10.csv\n",
      "Processing file: data\\C200\\C200_SEC11.csv\n",
      "Processing file: data\\C200\\C200_SEC12.csv\n",
      "Processing file: data\\C200\\C200_SEC13.csv\n",
      "Processing file: data\\C200\\C200_SEC14.csv\n",
      "Processing file: data\\C200\\C200_SEC15.csv\n",
      "Processing file: data\\C200\\C200_SEC16.csv\n",
      "Processing file: data\\C200\\C200_SEC17.csv\n",
      "Processing file: data\\C200\\C200_SEC18.csv\n",
      "Processing file: data\\C200\\C200_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for C200 saved to: combined_data\\C200_combined.csv\n",
      "Processing subdirectory: C300\n",
      "Processing file: data\\C300\\C300_SEC02.csv\n",
      "Processing file: data\\C300\\C300_SEC04.csv\n",
      "Processing file: data\\C300\\C300_SEC05.csv\n",
      "Processing file: data\\C300\\C300_SEC06.csv\n",
      "Processing file: data\\C300\\C300_SEC07.csv\n",
      "Processing file: data\\C300\\C300_SEC08.csv\n",
      "Processing file: data\\C300\\C300_SEC09.csv\n",
      "Processing file: data\\C300\\C300_SEC10.csv\n",
      "Processing file: data\\C300\\C300_SEC11.csv\n",
      "Processing file: data\\C300\\C300_SEC12.csv\n",
      "Processing file: data\\C300\\C300_SEC13.csv\n",
      "Processing file: data\\C300\\C300_SEC14.csv\n",
      "Processing file: data\\C300\\C300_SEC15.csv\n",
      "Processing file: data\\C300\\C300_SEC16.csv\n",
      "Processing file: data\\C300\\C300_SEC17.csv\n",
      "Processing file: data\\C300\\C300_SEC18.csv\n",
      "Processing file: data\\C300\\C300_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for C300 saved to: combined_data\\C300_combined.csv\n",
      "Processing subdirectory: C400\n",
      "Processing file: data\\C400\\C400_SEC02.csv\n",
      "Processing file: data\\C400\\C400_SEC04.csv\n",
      "Processing file: data\\C400\\C400_SEC05.csv\n",
      "Processing file: data\\C400\\C400_SEC06.csv\n",
      "Processing file: data\\C400\\C400_SEC07.csv\n",
      "Processing file: data\\C400\\C400_SEC08.csv\n",
      "Warning: Column count mismatch in data\\C400\\C400_SEC08.csv. Adjusting...\n",
      "Processing file: data\\C400\\C400_SEC09.csv\n",
      "Processing file: data\\C400\\C400_SEC10.csv\n",
      "Processing file: data\\C400\\C400_SEC11.csv\n",
      "Processing file: data\\C400\\C400_SEC12.csv\n",
      "Warning: Column count mismatch in data\\C400\\C400_SEC12.csv. Adjusting...\n",
      "Processing file: data\\C400\\C400_SEC13.csv\n",
      "Processing file: data\\C400\\C400_SEC14.csv\n",
      "Processing file: data\\C400\\C400_SEC15.csv\n",
      "Processing file: data\\C400\\C400_SEC16.csv\n",
      "Processing file: data\\C400\\C400_SEC17.csv\n",
      "Processing file: data\\C400\\C400_SEC18.csv\n",
      "Processing file: data\\C400\\C400_SEC19.csv\n",
      "Removed columns: username, learner_id, base_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_21704\\3421478998.py:109: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df = combined_df.replace({'Pass': 1, 'Fail': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined and processed data for C400 saved to: combined_data\\C400_combined.csv\n",
      "Processing subdirectory: C500\n",
      "Processing file: data\\C500\\C500_SEC02.csv\n",
      "Processing file: data\\C500\\C500_SEC04.csv\n",
      "Processing file: data\\C500\\C500_SEC05.csv\n",
      "Processing file: data\\C500\\C500_SEC06.csv\n",
      "Processing file: data\\C500\\C500_SEC07.csv\n",
      "Processing file: data\\C500\\C500_SEC08.csv\n",
      "Processing file: data\\C500\\C500_SEC09.csv\n",
      "Processing file: data\\C500\\C500_SEC10.csv\n",
      "Processing file: data\\C500\\C500_SEC11.csv\n",
      "Processing file: data\\C500\\C500_SEC12.csv\n",
      "Processing file: data\\C500\\C500_SEC13.csv\n",
      "Processing file: data\\C500\\C500_SEC14.csv\n",
      "Processing file: data\\C500\\C500_SEC15.csv\n",
      "Processing file: data\\C500\\C500_SEC16.csv\n",
      "Processing file: data\\C500\\C500_SEC17.csv\n",
      "Processing file: data\\C500\\C500_SEC18.csv\n",
      "Processing file: data\\C500\\C500_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for C500 saved to: combined_data\\C500_combined.csv\n",
      "Processing subdirectory: F100\n",
      "Processing file: data\\F100\\F100_SEC02.csv\n",
      "Processing file: data\\F100\\F100_SEC04.csv\n",
      "Processing file: data\\F100\\F100_SEC05.csv\n",
      "Processing file: data\\F100\\F100_SEC06.csv\n",
      "Processing file: data\\F100\\F100_SEC07.csv\n",
      "Processing file: data\\F100\\F100_SEC08.csv\n",
      "Warning: Column count mismatch in data\\F100\\F100_SEC08.csv. Adjusting...\n",
      "Processing file: data\\F100\\F100_SEC09.csv\n",
      "Processing file: data\\F100\\F100_SEC10.csv\n",
      "Processing file: data\\F100\\F100_SEC11.csv\n",
      "Processing file: data\\F100\\F100_SEC12.csv\n",
      "Processing file: data\\F100\\F100_SEC13.csv\n",
      "Processing file: data\\F100\\F100_SEC14.csv\n",
      "Processing file: data\\F100\\F100_SEC15.csv\n",
      "Processing file: data\\F100\\F100_SEC16.csv\n",
      "Warning: Column count mismatch in data\\F100\\F100_SEC16.csv. Adjusting...\n",
      "Processing file: data\\F100\\F100_SEC17.csv\n",
      "Processing file: data\\F100\\F100_SEC18.csv\n",
      "Processing file: data\\F100\\F100_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, base_id\n",
      "Combined and processed data for F100 saved to: combined_data\\F100_combined.csv\n",
      "Processing subdirectory: H100\n",
      "Processing file: data\\H100\\H100_SEC02.csv\n",
      "Processing file: data\\H100\\H100_SEC04.csv\n",
      "Processing file: data\\H100\\H100_SEC05.csv\n",
      "Processing file: data\\H100\\H100_SEC06.csv\n",
      "Warning: Column count mismatch in data\\H100\\H100_SEC06.csv. Adjusting...\n",
      "Processing file: data\\H100\\H100_SEC07.csv\n",
      "Processing file: data\\H100\\H100_SEC08.csv\n",
      "Warning: Column count mismatch in data\\H100\\H100_SEC08.csv. Adjusting...\n",
      "Processing file: data\\H100\\H100_SEC09.csv\n",
      "Warning: Column count mismatch in data\\H100\\H100_SEC09.csv. Adjusting...\n",
      "Processing file: data\\H100\\H100_SEC10.csv\n",
      "Processing file: data\\H100\\H100_SEC11.csv\n",
      "Processing file: data\\H100\\H100_SEC12.csv\n",
      "Processing file: data\\H100\\H100_SEC13.csv\n",
      "Processing file: data\\H100\\H100_SEC14.csv\n",
      "Warning: Column count mismatch in data\\H100\\H100_SEC14.csv. Adjusting...\n",
      "Processing file: data\\H100\\H100_SEC15.csv\n",
      "Warning: Column count mismatch in data\\H100\\H100_SEC15.csv. Adjusting...\n",
      "Processing file: data\\H100\\H100_SEC16.csv\n",
      "Processing file: data\\H100\\H100_SEC17.csv\n",
      "Processing file: data\\H100\\H100_SEC18.csv\n",
      "Processing file: data\\H100\\H100_SEC19.csv\n",
      "Warning: Column count mismatch in data\\H100\\H100_SEC19.csv. Adjusting...\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for H100 saved to: combined_data\\H100_combined.csv\n",
      "Processing subdirectory: H400\n",
      "Processing file: data\\H400\\H400_SEC02.csv\n",
      "Processing file: data\\H400\\H400_SEC04.csv\n",
      "Processing file: data\\H400\\H400_SEC05.csv\n",
      "Processing file: data\\H400\\H400_SEC06.csv\n",
      "Processing file: data\\H400\\H400_SEC07.csv\n",
      "Processing file: data\\H400\\H400_SEC08.csv\n",
      "Processing file: data\\H400\\H400_SEC09.csv\n",
      "Warning: Column count mismatch in data\\H400\\H400_SEC09.csv. Adjusting...\n",
      "Processing file: data\\H400\\H400_SEC10.csv\n",
      "Processing file: data\\H400\\H400_SEC11.csv\n",
      "Processing file: data\\H400\\H400_SEC12.csv\n",
      "Processing file: data\\H400\\H400_SEC13.csv\n",
      "Processing file: data\\H400\\H400_SEC14.csv\n",
      "Processing file: data\\H400\\H400_SEC15.csv\n",
      "Processing file: data\\H400\\H400_SEC16.csv\n",
      "Processing file: data\\H400\\H400_SEC17.csv\n",
      "Processing file: data\\H400\\H400_SEC18.csv\n",
      "Processing file: data\\H400\\H400_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for H400 saved to: combined_data\\H400_combined.csv\n",
      "Processing subdirectory: L100\n",
      "Processing file: data\\L100\\L100_SEC02.csv\n",
      "Processing file: data\\L100\\L100_SEC04.csv\n",
      "Processing file: data\\L100\\L100_SEC05.csv\n",
      "Processing file: data\\L100\\L100_SEC06.csv\n",
      "Processing file: data\\L100\\L100_SEC07.csv\n",
      "Processing file: data\\L100\\L100_SEC08.csv\n",
      "Processing file: data\\L100\\L100_SEC09.csv\n",
      "Processing file: data\\L100\\L100_SEC10.csv\n",
      "Processing file: data\\L100\\L100_SEC11.csv\n",
      "Warning: Column count mismatch in data\\L100\\L100_SEC11.csv. Adjusting...\n",
      "Processing file: data\\L100\\L100_SEC12.csv\n",
      "Processing file: data\\L100\\L100_SEC13.csv\n",
      "Processing file: data\\L100\\L100_SEC14.csv\n",
      "Processing file: data\\L100\\L100_SEC15.csv\n",
      "Processing file: data\\L100\\L100_SEC16.csv\n",
      "Processing file: data\\L100\\L100_SEC17.csv\n",
      "Processing file: data\\L100\\L100_SEC18.csv\n",
      "Processing file: data\\L100\\L100_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for L100 saved to: combined_data\\L100_combined.csv\n",
      "Processing subdirectory: L400\n",
      "Processing file: data\\L400\\L400_SEC02.csv\n",
      "Warning: Column count mismatch in data\\L400\\L400_SEC02.csv. Adjusting...\n",
      "Processing file: data\\L400\\L400_SEC04.csv\n",
      "Processing file: data\\L400\\L400_SEC05.csv\n",
      "Processing file: data\\L400\\L400_SEC06.csv\n",
      "Processing file: data\\L400\\L400_SEC07.csv\n",
      "Processing file: data\\L400\\L400_SEC08.csv\n",
      "Processing file: data\\L400\\L400_SEC09.csv\n",
      "Processing file: data\\L400\\L400_SEC10.csv\n",
      "Processing file: data\\L400\\L400_SEC11.csv\n",
      "Processing file: data\\L400\\L400_SEC12.csv\n",
      "Processing file: data\\L400\\L400_SEC13.csv\n",
      "Processing file: data\\L400\\L400_SEC14.csv\n",
      "Processing file: data\\L400\\L400_SEC15.csv\n",
      "Processing file: data\\L400\\L400_SEC16.csv\n",
      "Processing file: data\\L400\\L400_SEC17.csv\n",
      "Processing file: data\\L400\\L400_SEC18.csv\n",
      "Processing file: data\\L400\\L400_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for L400 saved to: combined_data\\L400_combined.csv\n",
      "Processing subdirectory: M000\n",
      "Processing file: data\\M000\\M000_SEC02.csv\n",
      "Processing file: data\\M000\\M000_SEC04.csv\n",
      "Processing file: data\\M000\\M000_SEC05.csv\n",
      "Processing file: data\\M000\\M000_SEC06.csv\n",
      "Processing file: data\\M000\\M000_SEC07.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_21704\\3421478998.py:109: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df = combined_df.replace({'Pass': 1, 'Fail': 0})\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_21704\\3421478998.py:109: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df = combined_df.replace({'Pass': 1, 'Fail': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data\\M000\\M000_SEC08.csv\n",
      "Processing file: data\\M000\\M000_SEC09.csv\n",
      "Processing file: data\\M000\\M000_SEC10.csv\n",
      "Processing file: data\\M000\\M000_SEC11.csv\n",
      "Processing file: data\\M000\\M000_SEC13.csv\n",
      "Processing file: data\\M000\\M000_SEC14.csv\n",
      "Processing file: data\\M000\\M000_SEC15.csv\n",
      "Processing file: data\\M000\\M000_SEC16.csv\n",
      "Processing file: data\\M000\\M000_SEC17.csv\n",
      "Processing file: data\\M000\\M000_SEC18.csv\n",
      "Processing file: data\\M000\\M000_SEC19.csv\n",
      "Removed columns: learner_id, username, last_access, availability, base_id\n",
      "Combined and processed data for M000 saved to: combined_data\\M000_combined.csv\n",
      "Processing subdirectory: M100\n",
      "Processing file: data\\M100\\M100_SEC02.csv\n",
      "Processing file: data\\M100\\M100_SEC04.csv\n",
      "Processing file: data\\M100\\M100_SEC05.csv\n",
      "Processing file: data\\M100\\M100_SEC06.csv\n",
      "Processing file: data\\M100\\M100_SEC07.csv\n",
      "Processing file: data\\M100\\M100_SEC08.csv\n",
      "Processing file: data\\M100\\M100_SEC09.csv\n",
      "Processing file: data\\M100\\M100_SEC10.csv\n",
      "Processing file: data\\M100\\M100_SEC11.csv\n",
      "Processing file: data\\M100\\M100_SEC12.csv\n",
      "Processing file: data\\M100\\M100_SEC13.csv\n",
      "Processing file: data\\M100\\M100_SEC14.csv\n",
      "Processing file: data\\M100\\M100_SEC15.csv\n",
      "Processing file: data\\M100\\M100_SEC16.csv\n",
      "Processing file: data\\M100\\M100_SEC17.csv\n",
      "Processing file: data\\M100\\M100_SEC18.csv\n",
      "Processing file: data\\M100\\M100_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for M100 saved to: combined_data\\M100_combined.csv\n",
      "Processing subdirectory: M200\n",
      "Processing file: data\\M200\\M200_SEC013.csv\n",
      "Processing file: data\\M200\\M200_SEC02.csv\n",
      "Processing file: data\\M200\\M200_SEC04.csv\n",
      "Processing file: data\\M200\\M200_SEC05.csv\n",
      "Processing file: data\\M200\\M200_SEC06.csv\n",
      "Processing file: data\\M200\\M200_SEC07.csv\n",
      "Processing file: data\\M200\\M200_SEC08.csv\n",
      "Processing file: data\\M200\\M200_SEC09.csv\n",
      "Processing file: data\\M200\\M200_SEC10.csv\n",
      "Processing file: data\\M200\\M200_SEC11.csv\n",
      "Processing file: data\\M200\\M200_SEC12.csv\n",
      "Processing file: data\\M200\\M200_SEC14.csv\n",
      "Processing file: data\\M200\\M200_SEC15.csv\n",
      "Processing file: data\\M200\\M200_SEC16.csv\n",
      "Processing file: data\\M200\\M200_SEC17.csv\n",
      "Processing file: data\\M200\\M200_SEC18.csv\n",
      "Processing file: data\\M200\\M200_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for M200 saved to: combined_data\\M200_combined.csv\n",
      "Processing subdirectory: M300\n",
      "Processing file: data\\M300\\M300_SEC02.csv\n",
      "Processing file: data\\M300\\M300_SEC04.csv\n",
      "Processing file: data\\M300\\M300_SEC05.csv\n",
      "Processing file: data\\M300\\M300_SEC06.csv\n",
      "Processing file: data\\M300\\M300_SEC07.csv\n",
      "Processing file: data\\M300\\M300_SEC08.csv\n",
      "Processing file: data\\M300\\M300_SEC09.csv\n",
      "Processing file: data\\M300\\M300_SEC10.csv\n",
      "Processing file: data\\M300\\M300_SEC11.csv\n",
      "Processing file: data\\M300\\M300_SEC12.csv\n",
      "Processing file: data\\M300\\M300_SEC13.csv\n",
      "Processing file: data\\M300\\M300_SEC14.csv\n",
      "Processing file: data\\M300\\M300_SEC15.csv\n",
      "Processing file: data\\M300\\M300_SEC16.csv\n",
      "Processing file: data\\M300\\M300_SEC17.csv\n",
      "Processing file: data\\M300\\M300_SEC18.csv\n",
      "Processing file: data\\M300\\M300_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for M300 saved to: combined_data\\M300_combined.csv\n",
      "Processing subdirectory: M400\n",
      "Processing file: data\\M400\\M400_SEC02.csv\n",
      "Processing file: data\\M400\\M400_SEC04.csv\n",
      "Processing file: data\\M400\\M400_SEC05.csv\n",
      "Processing file: data\\M400\\M400_SEC06.csv\n",
      "Processing file: data\\M400\\M400_SEC07.csv\n",
      "Processing file: data\\M400\\M400_SEC08.csv\n",
      "Processing file: data\\M400\\M400_SEC09.csv\n",
      "Processing file: data\\M400\\M400_SEC10.csv\n",
      "Processing file: data\\M400\\M400_SEC11.csv\n",
      "Processing file: data\\M400\\M400_SEC12.csv\n",
      "Processing file: data\\M400\\M400_SEC13.csv\n",
      "Processing file: data\\M400\\M400_SEC14.csv\n",
      "Processing file: data\\M400\\M400_SEC15.csv\n",
      "Processing file: data\\M400\\M400_SEC16.csv\n",
      "Processing file: data\\M400\\M400_SEC17.csv\n",
      "Processing file: data\\M400\\M400_SEC18.csv\n",
      "Processing file: data\\M400\\M400_SEC19.csv\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for M400 saved to: combined_data\\M400_combined.csv\n",
      "Processing subdirectory: S100\n",
      "Processing file: data\\S100\\S100_SEC02.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC02.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC04.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC04.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC05.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC05.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC06.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC06.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC07.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC07.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC08.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC08.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC09.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC09.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC10.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC10.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC11.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC11.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC12.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC12.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC13.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC13.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC14.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC14.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC15.csv\n",
      "Processing file: data\\S100\\S100_SEC16.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC16.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC17.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC17.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC18.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC18.csv. Adjusting...\n",
      "Processing file: data\\S100\\S100_SEC19.csv\n",
      "Warning: Column count mismatch in data\\S100\\S100_SEC19.csv. Adjusting...\n",
      "Removed columns: username, learner_id, last_access, availability, base_id\n",
      "Combined and processed data for S100 saved to: combined_data\\S100_combined.csv\n",
      "Processing subdirectory: X100\n",
      "Error: 'learner_id' column not found in X100.\n",
      "Available columns: ['Last Name', 'First Name', 'Learner ID', 'Username', 'Last Access', 'Availability', 'X102 Board Learner Acknowledgement of Recording Privacy Act Satement [Total Pts: 100 Complete/Incomplete] |402665', 'X100A Oral Board [Total Pts: 100 Score] |402662', 'X100B Online Exam [Total Pts: 100 Score] |402663', 'X100 Weighted Total [Total Pts: up to 100 Percentage] |402664', 'Average X100 Oral Board [Total Pts: up to 100 Percentage] |402666', '#1Chairperson [Total Pts: 100 Score] |402667', '#2 Instructor [Total Pts: 100 Score] |402669', '#3 Instructor [Total Pts: 100 Score] |402670', 'Ave of PreTest [Total Pts: up to 100 Score] |402671', 'Weighted Total [Total Pts: up to 0 Percentage] |402672', 'AY22_X100_Pretest (IMS) [Total Pts: 100 Score] |402674', 'Re-Test - #1 Chairperson [Total Pts: 100 Score] |402678', 'Re-Test - #2 Instructor [Total Pts: 100 Score] |402679', 'Re-Test- #3 Instructor [Total Pts: 100 Score] |402680', 'AY24_X102_Online Exam (US) [Total Pts: 100 Score] |422559', 'AY24_X102_Online Exam (IMS) [Total Pts: 100 Score] |422560']\n",
      "Finished processing all subdirectories.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "def load_grading_scale(file_path):\n",
    "    grading_df = pd.read_csv(file_path)\n",
    "    grade_map = dict(zip(grading_df['Letter Grade'], grading_df['4-Point Equivalence']))\n",
    "    return grade_map\n",
    "\n",
    "def convert_grade(value, grade_map, column_name):\n",
    "    columns_to_ignore = ['section', 'team', 'unique_id']\n",
    "    if column_name.lower() in columns_to_ignore:\n",
    "        return value\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        return grade_map.get(value.upper(), value)\n",
    "    return value\n",
    "\n",
    "def process_csv_files(base_path, output_path, grading_scale_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    grade_map = load_grading_scale(grading_scale_path)\n",
    "    \n",
    "    subdirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        print(f\"Processing subdirectory: {subdir}\")\n",
    "        csv_files = sorted(glob(os.path.join(base_path, subdir, '*.csv')))\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in {os.path.join(base_path, subdir)}\")\n",
    "            continue\n",
    "\n",
    "        # Read the first file to get the correct column names\n",
    "        first_file = csv_files[0]\n",
    "        first_df = pd.read_csv(first_file)\n",
    "        correct_columns = first_df.columns.tolist()\n",
    "        num_columns = len(correct_columns)\n",
    "\n",
    "        subdir_dataframes = [first_df]  # Include the first dataframe with headers\n",
    "\n",
    "        for file in csv_files[1:]:  # Start from the second file\n",
    "            print(f\"Processing file: {file}\")\n",
    "            \n",
    "            # Read the CSV file without headers, skipping the first row\n",
    "            df = pd.read_csv(file, header=None, skiprows=[0])\n",
    "            \n",
    "            # Check if the number of columns matches\n",
    "            if len(df.columns) != num_columns:\n",
    "                print(f\"Warning: Column count mismatch in {file}. Adjusting...\")\n",
    "                if len(df.columns) > num_columns:\n",
    "                    # If there are extra columns, drop them\n",
    "                    df = df.iloc[:, :num_columns]\n",
    "                else:\n",
    "                    # If there are missing columns, add them with NaN values\n",
    "                    for i in range(len(df.columns), num_columns):\n",
    "                        df[i] = np.nan\n",
    "            \n",
    "            # Assign the correct column names\n",
    "            df.columns = correct_columns\n",
    "\n",
    "            subdir_dataframes.append(df)\n",
    "\n",
    "        # Combine all dataframes for this subdirectory\n",
    "        combined_df = pd.concat(subdir_dataframes, ignore_index=True)\n",
    "\n",
    "        combined_df.dropna(how='all', inplace=True)\n",
    "\n",
    "        if 'learner_id' not in combined_df.columns:\n",
    "            print(f\"Error: 'learner_id' column not found in {subdir}.\")\n",
    "            print(\"Available columns:\", combined_df.columns.tolist())\n",
    "            continue\n",
    "\n",
    "        combined_df['team'] = pd.to_numeric(combined_df['learner_id'].str[:2], errors='coerce').astype('Int64')\n",
    "        combined_df['section'] = combined_df['learner_id'].str[-1]\n",
    "\n",
    "        if 'last_name' in combined_df.columns and 'first_name' in combined_df.columns:\n",
    "            combined_df['base_id'] = (combined_df['last_name'].str[0].fillna('') + \n",
    "                                      combined_df['first_name'].str[0].fillna('') + \n",
    "                                      combined_df['learner_id'].fillna(''))\n",
    "        else:\n",
    "            print(f\"Warning: 'last_name' or 'first_name' columns not found in {subdir}. Using only 'learner_id' for base_id.\")\n",
    "            combined_df['base_id'] = combined_df['learner_id'].fillna('')\n",
    "\n",
    "        # Create unique_id without using apply\n",
    "        combined_df['unique_id'] = combined_df.groupby('base_id').cumcount()\n",
    "        combined_df['unique_id'] = combined_df.apply(lambda row: f\"{row['base_id']}_{row['unique_id'] + 1}\" if row['unique_id'] > 0 else row['base_id'], axis=1)\n",
    "\n",
    "        # Case-insensitive column removal\n",
    "        columns_to_remove = ['availability', 'username', 'learner_id', 'base_id', 'last_access']\n",
    "        columns_to_remove_lower = [col.lower() for col in columns_to_remove]\n",
    "\n",
    "        columns_removed = []\n",
    "        for col in combined_df.columns:\n",
    "            if col.lower() in columns_to_remove_lower:\n",
    "                combined_df.drop(col, axis=1, inplace=True)\n",
    "                columns_removed.append(col)\n",
    "\n",
    "        if columns_removed:\n",
    "            print(f\"Removed columns: {', '.join(columns_removed)}\")\n",
    "        else:\n",
    "            print(\"No columns were removed.\")\n",
    "\n",
    "        # Round numeric columns to 2 decimal places\n",
    "        numeric_columns = combined_df.select_dtypes(include=[np.number]).columns\n",
    "        combined_df[numeric_columns] = combined_df[numeric_columns].round(2)\n",
    "\n",
    "        # Convert 'pass' to 1 and 'fail' to 0\n",
    "        combined_df = combined_df.replace({'Pass': 1, 'Fail': 0})\n",
    "\n",
    "        # Apply grading scale conversion, ignoring specified columns\n",
    "        for column in combined_df.columns:\n",
    "            combined_df[column] = combined_df[column].apply(lambda x: convert_grade(x, grade_map, column))\n",
    "\n",
    "        columns = combined_df.columns.tolist()\n",
    "        columns.remove('unique_id')\n",
    "        columns = ['unique_id'] + columns\n",
    "        combined_df = combined_df[columns]\n",
    "\n",
    "        # Save the combined dataframe for this subdirectory\n",
    "        output_file = os.path.join(output_path, f\"{subdir}_combined.csv\")\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined and processed data for {subdir} saved to: {output_file}\")\n",
    "\n",
    "    print(\"Finished processing all subdirectories.\")\n",
    "\n",
    "# Usage\n",
    "base_path = 'data'\n",
    "output_path = 'combined_data'\n",
    "grading_scale_path = 'grading_scale.csv'  # Make sure this file exists\n",
    "process_csv_files(base_path, output_path, grading_scale_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

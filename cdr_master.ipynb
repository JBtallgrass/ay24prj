{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path):\n",
    "    # Get all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {folder_path}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Read the first CSV file to get the column headers\n",
    "    initial_df = pd.read_csv(os.path.join(folder_path, csv_files[0]))\n",
    "    correct_columns = initial_df.columns\n",
    "\n",
    "    # Process the first file\n",
    "    dataframes.append(initial_df)\n",
    "    print(f\"Read {csv_files[0]} successfully with {len(initial_df)} rows.\")\n",
    "\n",
    "    # Loop through the remaining files\n",
    "    for file in csv_files[1:]:\n",
    "        try:\n",
    "            # Read the current CSV file into a DataFrame\n",
    "            df = pd.read_csv(os.path.join(folder_path, file))\n",
    "            \n",
    "            # Check if the number of columns matches\n",
    "            if len(df.columns) != len(correct_columns):\n",
    "                print(f\"Warning: {file} has {len(df.columns)} columns instead of {len(correct_columns)}.\")\n",
    "                print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "                print(f\"Expected columns: {correct_columns.tolist()}\")\n",
    "                continue  # Skip this file and move to the next one\n",
    "            \n",
    "            # Rename columns to match the correct columns\n",
    "            df.columns = correct_columns\n",
    "            \n",
    "            print(f\"Read {file} successfully with {len(df)} rows.\")\n",
    "            \n",
    "            # Drop rows where all cells are blank\n",
    "            df.dropna(how='all', inplace=True)\n",
    "            \n",
    "            # Append the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file}: {e}\")\n",
    "\n",
    "    # Check if there are any DataFrames to concatenate\n",
    "    if dataframes:\n",
    "        # Concatenate all DataFrames in the list into a single DataFrame\n",
    "        folder_df = pd.concat(dataframes, ignore_index=True)\n",
    "        \n",
    "        print(f\"Columns in the concatenated DataFrame: {folder_df.columns.tolist()}\")\n",
    "        \n",
    "        # Check if 'learner_id' column exists\n",
    "        if 'learner_id' not in folder_df.columns:\n",
    "            print(\"Error: 'learner_id' column not found in the DataFrame.\")\n",
    "            print(\"Available columns:\", folder_df.columns.tolist())\n",
    "            return None\n",
    "        \n",
    "        # Process the learner_id column\n",
    "        folder_df['team'] = pd.to_numeric(folder_df['learner_id'].str[:2], errors='coerce').astype('Int64')\n",
    "        folder_df['section'] = folder_df['learner_id'].str[-1]\n",
    "        \n",
    "        # Check if 'last_name' and 'first_name' columns exist\n",
    "        if 'last_name' in folder_df.columns and 'first_name' in folder_df.columns:\n",
    "            # Create a base for the unique identifier\n",
    "            folder_df['base_id'] = (folder_df['last_name'].str[0].fillna('') + \n",
    "                                    folder_df['first_name'].str[0].fillna('') + \n",
    "                                    folder_df['learner_id'].fillna(''))\n",
    "        else:\n",
    "            print(\"Warning: 'last_name' or 'first_name' columns not found. Using only 'learner_id' for base_id.\")\n",
    "            folder_df['base_id'] = folder_df['learner_id'].fillna('')\n",
    "        \n",
    "        # Function to create a truly unique identifier\n",
    "        def create_unique_id(group):\n",
    "            if len(group) == 1:\n",
    "                return group['base_id']\n",
    "            else:\n",
    "                return group['base_id'] + '_' + (group.groupby('base_id').cumcount() + 1).astype(str)\n",
    "        \n",
    "        # Apply the function to create unique identifiers\n",
    "        folder_df['unique_id'] = folder_df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
    "        \n",
    "        # Reorder columns to move unique_id to the leftmost position\n",
    "        columns = folder_df.columns.tolist()\n",
    "        columns.remove('unique_id')\n",
    "        columns = ['unique_id'] + columns\n",
    "        folder_df = folder_df[columns]\n",
    "        \n",
    "        return folder_df\n",
    "    else:\n",
    "        print(f\"No dataframes to concatenate in {folder_path}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total folders found: 4\n",
      "Folders to be processed:\n",
      "- data\\data\n",
      "- data\\GPA\n",
      "- data\\H100\n",
      "- data\\H400\n",
      "\n",
      "Skipping unexpected folder: data\\data\n",
      "\n",
      "Skipping unexpected folder: data\\GPA\n",
      "\n",
      "Processing folder: data\\H100\n",
      "Read H100_SEC01.csv successfully with 59 rows.\n",
      "Columns in the concatenated DataFrame: ['last_name', 'first_name', 'username', 'learner_id', 'last_access', 'availability', 'h100_wt_ttl', 'h100_ttl', 'h100_ctgl', 'h100_outline', 'h100_essay']\n",
      "Data compiled and saved successfully to cleaned_data\\H100_compiled_dataframe.csv\n",
      "Total rows in compiled dataframe: 59\n",
      "\n",
      "Summary of rows per section:\n",
      "section\n",
      "A    15\n",
      "B    15\n",
      "C    15\n",
      "D    14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data with reordered columns:\n",
      "  unique_id     last_name                first_name               username  \\\n",
      "0     AM01C        ACOSTA                  MITCHELL         mitch.f.acosta   \n",
      "1     AJ01A         ALLEN                    JUSTIN       justin.lee.allen   \n",
      "2     AK01A       ALLISON                     KEVIN        kevin.e.allison   \n",
      "3     AA01D     ALSUWAIDI  AHMED SAIF ABDULLA AHMED   alsuwaidi@hotmail.fr   \n",
      "4     AA01C      ANDERSON                      ALEX        alex.r.anderson   \n",
      "5     AA01B      ANDERSON                   ANTHONY    anthony.s.anderson2   \n",
      "6     AR01D      ANDERSON                    ROBERT  robert.edwin.anderson   \n",
      "7   AS01C_1      ANDERSON                    STACEY      stacey.c.anderson   \n",
      "8     AS01B      ANDERSON                   STEPHEN      stephen.anderson8   \n",
      "9     AJ01C  ANDRADE LAZO              JOSE ARMANDO    joseaal@hotmail.com   \n",
      "\n",
      "  learner_id       last_access availability  h100_wt_ttl  h100_ttl  h100_ctgl  \\\n",
      "0        01C   2/27/2024 14:44          Yes         95.7       291         95   \n",
      "1        01A    3/4/2024 19:21          Yes         95.0       290         95   \n",
      "2        01A    11/8/2023 8:33          Yes         93.6       288         95   \n",
      "3        01D    4/4/2024 12:55          Yes         94.2       286         90   \n",
      "4        01C     4/5/2024 8:24          Yes         91.5       285         95   \n",
      "5        01B   1/20/2024 21:26          Yes         95.0       290         95   \n",
      "6        01D  12/13/2023 10:37          Yes         93.3       287         94   \n",
      "7        01C   4/10/2024 12:50          Yes         90.5       283         94   \n",
      "8        01B   12/6/2023 10:32          Yes         94.3       289         95   \n",
      "9        01C    4/30/2024 7:54          Yes         93.6       288         95   \n",
      "\n",
      "  h100_outline  h100_essay  team section base_id  \n",
      "0         Pass          96     1       C   AM01C  \n",
      "1         Pass          95     1       A   AJ01A  \n",
      "2         Pass          93     1       A   AK01A  \n",
      "3         Pass          96     1       D   AA01D  \n",
      "4         Pass          90     1       C   AA01C  \n",
      "5         Pass          95     1       B   AA01B  \n",
      "6         Pass          93     1       D   AR01D  \n",
      "7         Pass          89     1       C   AS01C  \n",
      "8         Pass          94     1       B   AS01B  \n",
      "9         Pass          93     1       C   AJ01C  \n",
      "\n",
      "Processing folder: data\\H400\n",
      "Read H400_SEC01.csv successfully with 59 rows.\n",
      "Columns in the concatenated DataFrame: ['last_name', 'first_name', 'username', 'learner_id', 'last_access', 'availability', 'h400_wt_ttl', 'h400_ttl', 'h400_outline', 'h400_essay', 'h400_ctgl']\n",
      "Data compiled and saved successfully to cleaned_data\\H400_compiled_dataframe.csv\n",
      "Total rows in compiled dataframe: 59\n",
      "\n",
      "Summary of rows per section:\n",
      "section\n",
      "A    15\n",
      "B    15\n",
      "C    15\n",
      "D    14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data with reordered columns:\n",
      "  unique_id     last_name                first_name               username  \\\n",
      "0     AM01C        ACOSTA                  MITCHELL         mitch.f.acosta   \n",
      "1     AJ01A         ALLEN                    JUSTIN       justin.lee.allen   \n",
      "2     AK01A       ALLISON                     KEVIN        kevin.e.allison   \n",
      "3     AA01D     ALSUWAIDI  AHMED SAIF ABDULLA AHMED   alsuwaidi@hotmail.fr   \n",
      "4     AA01C      ANDERSON                      ALEX        alex.r.anderson   \n",
      "5     AA01B      ANDERSON                   ANTHONY    anthony.s.anderson2   \n",
      "6     AR01D      ANDERSON                    ROBERT  robert.edwin.anderson   \n",
      "7   AS01C_1      ANDERSON                    STACEY      stacey.c.anderson   \n",
      "8     AS01B      ANDERSON                   STEPHEN      stephen.anderson8   \n",
      "9     AJ01C  ANDRADE LAZO              JOSE ARMANDO    joseaal@hotmail.com   \n",
      "\n",
      "  learner_id      last_access availability  h400_wt_ttl  h400_ttl  \\\n",
      "0        01C   3/18/2024 8:14          Yes         95.9       293   \n",
      "1        01A  3/18/2024 11:18          Yes         94.0       288   \n",
      "2        01A   3/12/2024 7:40          Yes         91.2       284   \n",
      "3        01D   3/19/2024 9:09          Yes         95.6       292   \n",
      "4        01C    4/5/2024 8:24          Yes         92.4       288   \n",
      "5        01B  3/11/2024 16:27          Yes         95.8       294   \n",
      "6        01D   3/21/2024 8:27          Yes         90.8       284   \n",
      "7        01C  4/22/2024 12:48          Yes         96.5       295   \n",
      "8        01B    3/5/2024 9:49          Yes         97.3       295   \n",
      "9        01C   4/30/2024 7:54          Yes         90.6       278   \n",
      "\n",
      "  h400_outline  h400_essay  h400_ctgl  team section base_id  \n",
      "0         Pass        95.0       98.0     1       C   AM01C  \n",
      "1         Pass        94.0       94.0     1       A   AJ01A  \n",
      "2         Pass        90.0       94.0     1       A   AK01A  \n",
      "3         Pass        95.0       97.0     1       D   AA01D  \n",
      "4         Pass        90.0       98.0     1       C   AA01C  \n",
      "5         Pass        94.0      100.0     1       B   AA01B  \n",
      "6         Pass        89.0       95.0     1       D   AR01D  \n",
      "7         Pass        95.0      100.0     1       C   AS01C  \n",
      "8         Pass        97.0       98.0     1       B   AS01B  \n",
      "9         Pass        93.0       85.0     1       C   AJ01C  \n",
      "\n",
      "Total folders processed: 2\n",
      "Expected folders: 16\n",
      "Missing folders: 14\n",
      "\n",
      "Processed folders:\n",
      "- H100\n",
      "- H400\n",
      "\n",
      "Missing folders:\n",
      "- C100\n",
      "- C200\n",
      "- C300\n",
      "- C400\n",
      "- C500\n",
      "- F100\n",
      "- S100\n",
      "- L100\n",
      "- L400\n",
      "- M000\n",
      "- M100\n",
      "- M200\n",
      "- M300\n",
      "- M400\n",
      "\n",
      "All available folders processed.\n",
      "\n",
      "Contents of the cleaned_data folder:\n",
      "- c100_compiled_dataframe.csv\n",
      "- c200_compiled_dataframe.csv\n",
      "- C300_compiled_dataframe.csv\n",
      "- C400_compiled_dataframe.csv\n",
      "- C500_compiled_dataframe.csv\n",
      "- F100_compiled_dataframe.csv\n",
      "- H100_compiled_dataframe.csv\n",
      "- H400_compiled_dataframe.csv\n",
      "- L100_compiled_dataframe.csv\n",
      "- L400_compiled_dataframe.csv\n",
      "- M000_compiled_dataframe.csv\n",
      "- M100_compiled_dataframe.csv\n",
      "- M200_compiled_dataframe.csv\n",
      "- M300_compiled_dataframe.csv\n",
      "- M400_compiled_dataframe.csv\n",
      "- S100_compiled_dataframe.csv\n",
      "- test_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_8916\\2232016110.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  folder_df['unique_id'] = folder_df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_8916\\2232016110.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  folder_df['unique_id'] = folder_df.groupby('base_id', group_keys=False).apply(create_unique_id)\n"
     ]
    }
   ],
   "source": [
    "# Main script\n",
    "data_root = 'data'  # Adjust this to your data root directory\n",
    "output_dir = 'cleaned_data'  # Directory to save output files\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Expected folder names\n",
    "expected_folders = [\n",
    "    'C100', 'C200', 'C300', 'C400', 'C500', \n",
    "    'F100', 'S100', 'H100', 'H400', 'L100', 'L400', \n",
    "    'M000', 'M100', 'M200', 'M300', 'M400'\n",
    "]\n",
    "\n",
    "# Get all subdirectories in the data root\n",
    "data_folders = [f.path for f in os.scandir(data_root) if f.is_dir()]\n",
    "\n",
    "print(f\"Total folders found: {len(data_folders)}\")\n",
    "print(\"Folders to be processed:\")\n",
    "for folder in data_folders:\n",
    "    print(f\"- {folder}\")\n",
    "\n",
    "# Process each folder\n",
    "processed_folders = 0\n",
    "processed_folder_names = []\n",
    "\n",
    "for folder in data_folders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    if folder_name not in expected_folders:\n",
    "        print(f\"\\nSkipping unexpected folder: {folder}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing folder: {folder}\")\n",
    "    result_df = process_folder(folder)\n",
    "    \n",
    "    if result_df is not None:\n",
    "        # Save the compiled DataFrame\n",
    "        output_file = os.path.join(output_dir, f'{folder_name}_compiled_dataframe.csv')\n",
    "        result_df.to_csv(output_file, index=False)\n",
    "        print(f\"Data compiled and saved successfully to {output_file}\")\n",
    "        print(f\"Total rows in compiled dataframe: {len(result_df)}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary of rows per section:\")\n",
    "        print(result_df['section'].value_counts().sort_index())\n",
    "        \n",
    "        # Display the first few rows to verify the new column order\n",
    "        print(\"\\nSample data with reordered columns:\")\n",
    "        print(result_df.head(10))\n",
    "\n",
    "        processed_folders += 1\n",
    "        processed_folder_names.append(folder_name)\n",
    "    else:\n",
    "        print(f\"No data processed for {folder}\")\n",
    "\n",
    "print(f\"\\nTotal folders processed: {processed_folders}\")\n",
    "print(f\"Expected folders: {len(expected_folders)}\")\n",
    "print(f\"Missing folders: {len(expected_folders) - processed_folders}\")\n",
    "\n",
    "print(\"\\nProcessed folders:\")\n",
    "for folder in processed_folder_names:\n",
    "    print(f\"- {folder}\")\n",
    "\n",
    "print(\"\\nMissing folders:\")\n",
    "for folder in expected_folders:\n",
    "    if folder not in processed_folder_names:\n",
    "        print(f\"- {folder}\")\n",
    "\n",
    "print(\"\\nAll available folders processed.\")\n",
    "\n",
    "# Check the contents of the cleaned_data folder\n",
    "print(\"\\nContents of the cleaned_data folder:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    print(f\"- {file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score \n",
    "from glob import glob   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grading_scale.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with the grading information\n",
    "data = {\n",
    "    \"Letter Grade\": [\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"C+\", \"C\", \"U\"],\n",
    "    \"4-Point Equivalence\": [4.30, 4.00, 3.67, 3.33, 3.00, 2.33, 2.00, 0.00],\n",
    "    \"Grading Range\": [\"97 - 100\", \"94 - 96.99\", \"90 - 93.99\", \"87 - 89.99\", \"80 - 86.99\", \"77 - 79.99\", \"70 - 76.99\", \"<70\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export the dataframe to a CSV file\n",
    "csv_file_path = \"grading_scale.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "csv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data\\C100\\C100_SEC01.csv successfully with 59 rows.\n",
      "Read data\\C100\\C100_SEC02.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC04.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC05.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC06.csv successfully with 61 rows.\n",
      "Read data\\C100\\C100_SEC07.csv successfully with 59 rows.\n",
      "Read data\\C100\\C100_SEC08.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC09.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC10.csv successfully with 59 rows.\n",
      "Read data\\C100\\C100_SEC11.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC12.csv successfully with 59 rows.\n",
      "Read data\\C100\\C100_SEC13.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC14.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC15.csv successfully with 61 rows.\n",
      "Read data\\C100\\C100_SEC16.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC17.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC18.csv successfully with 60 rows.\n",
      "Read data\\C100\\C100_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in C100. Total rows: 1078. Saved to combined_data\\C100_combined.csv\n",
      "Read data\\C200\\C200_SEC01.csv successfully with 59 rows.\n",
      "Read data\\C200\\C200_SEC02.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC04.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC05.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC06.csv successfully with 61 rows.\n",
      "Read data\\C200\\C200_SEC07.csv successfully with 59 rows.\n",
      "Read data\\C200\\C200_SEC08.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC09.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC10.csv successfully with 59 rows.\n",
      "Read data\\C200\\C200_SEC11.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC12.csv successfully with 59 rows.\n",
      "Read data\\C200\\C200_SEC13.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC14.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC15.csv successfully with 61 rows.\n",
      "Read data\\C200\\C200_SEC16.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC17.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC18.csv successfully with 60 rows.\n",
      "Read data\\C200\\C200_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in C200. Total rows: 1078. Saved to combined_data\\C200_combined.csv\n",
      "Read data\\C300\\C300_SEC01.csv successfully with 59 rows.\n",
      "Read data\\C300\\C300_SEC02.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC04.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC05.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC06.csv successfully with 61 rows.\n",
      "Read data\\C300\\C300_SEC07.csv successfully with 59 rows.\n",
      "Read data\\C300\\C300_SEC08.csv successfully with 61 rows.\n",
      "Read data\\C300\\C300_SEC09.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC10.csv successfully with 59 rows.\n",
      "Read data\\C300\\C300_SEC11.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC12.csv successfully with 59 rows.\n",
      "Read data\\C300\\C300_SEC13.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC14.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC15.csv successfully with 61 rows.\n",
      "Read data\\C300\\C300_SEC16.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC17.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC18.csv successfully with 60 rows.\n",
      "Read data\\C300\\C300_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in C300. Total rows: 1079. Saved to combined_data\\C300_combined.csv\n",
      "Read data\\C400\\C400_SEC01.csv successfully with 59 rows.\n",
      "Read data\\C400\\C400_SEC02.csv successfully with 61 rows.\n",
      "Read data\\C400\\C400_SEC04.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC05.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC06.csv successfully with 61 rows.\n",
      "Read data\\C400\\C400_SEC07.csv successfully with 59 rows.\n",
      "Read data\\C400\\C400_SEC08.csv successfully with 61 rows.\n",
      "Read data\\C400\\C400_SEC09.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC10.csv successfully with 59 rows.\n",
      "Read data\\C400\\C400_SEC11.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC12.csv successfully with 59 rows.\n",
      "Read data\\C400\\C400_SEC13.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC14.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC15.csv successfully with 61 rows.\n",
      "Read data\\C400\\C400_SEC16.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC17.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC18.csv successfully with 60 rows.\n",
      "Read data\\C400\\C400_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in C400. Total rows: 1080. Saved to combined_data\\C400_combined.csv\n",
      "Read data\\C500\\C500_SEC01.csv successfully with 59 rows.\n",
      "Read data\\C500\\C500_SEC02.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC04.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC05.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC06.csv successfully with 61 rows.\n",
      "Read data\\C500\\C500_SEC07.csv successfully with 59 rows.\n",
      "Read data\\C500\\C500_SEC08.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC09.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC10.csv successfully with 59 rows.\n",
      "Read data\\C500\\C500_SEC11.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC12.csv successfully with 59 rows.\n",
      "Read data\\C500\\C500_SEC13.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC14.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC15.csv successfully with 61 rows.\n",
      "Read data\\C500\\C500_SEC16.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC17.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC18.csv successfully with 60 rows.\n",
      "Read data\\C500\\C500_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in C500. Total rows: 1078. Saved to combined_data\\C500_combined.csv\n",
      "Read data\\F100\\F100_SEC01.csv successfully with 59 rows.\n",
      "Read data\\F100\\F100_SEC02.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC04.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC05.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC06.csv successfully with 61 rows.\n",
      "Read data\\F100\\F100_SEC07.csv successfully with 59 rows.\n",
      "Read data\\F100\\F100_SEC08.csv successfully with 61 rows.\n",
      "Read data\\F100\\F100_SEC09.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC10.csv successfully with 59 rows.\n",
      "Read data\\F100\\F100_SEC11.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC12.csv successfully with 59 rows.\n",
      "Read data\\F100\\F100_SEC13.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC14.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC15.csv successfully with 61 rows.\n",
      "Read data\\F100\\F100_SEC16.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC17.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC18.csv successfully with 60 rows.\n",
      "Read data\\F100\\F100_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in F100. Total rows: 1079. Saved to combined_data\\F100_combined.csv\n",
      "Read data\\H100\\H100_SEC01.csv successfully with 59 rows.\n",
      "Read data\\H100\\H100_SEC02.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC04.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC05.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC06.csv successfully with 61 rows.\n",
      "Read data\\H100\\H100_SEC07.csv successfully with 59 rows.\n",
      "Read data\\H100\\H100_SEC08.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC09.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC10.csv successfully with 59 rows.\n",
      "Read data\\H100\\H100_SEC11.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC12.csv successfully with 59 rows.\n",
      "Read data\\H100\\H100_SEC13.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC14.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC15.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC16.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC17.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC18.csv successfully with 60 rows.\n",
      "Read data\\H100\\H100_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in H100. Total rows: 1077. Saved to combined_data\\H100_combined.csv\n",
      "Read data\\H400\\H400_SEC01.csv successfully with 59 rows.\n",
      "Read data\\H400\\H400_SEC02.csv successfully with 56 rows.\n",
      "Read data\\H400\\H400_SEC04.csv successfully with 59 rows.\n",
      "Read data\\H400\\H400_SEC05.csv successfully with 60 rows.\n",
      "Read data\\H400\\H400_SEC06.csv successfully with 59 rows.\n",
      "Read data\\H400\\H400_SEC07.csv successfully with 58 rows.\n",
      "Read data\\H400\\H400_SEC08.csv successfully with 59 rows.\n",
      "Read data\\H400\\H400_SEC09.csv successfully with 57 rows.\n",
      "Read data\\H400\\H400_SEC10.csv successfully with 58 rows.\n",
      "Read data\\H400\\H400_SEC11.csv successfully with 60 rows.\n",
      "Read data\\H400\\H400_SEC12.csv successfully with 59 rows.\n",
      "Read data\\H400\\H400_SEC13.csv successfully with 59 rows.\n",
      "Read data\\H400\\H400_SEC14.csv successfully with 57 rows.\n",
      "Read data\\H400\\H400_SEC15.csv successfully with 60 rows.\n",
      "Read data\\H400\\H400_SEC16.csv successfully with 59 rows.\n",
      "Read data\\H400\\H400_SEC17.csv successfully with 59 rows.\n",
      "Read data\\H400\\H400_SEC18.csv successfully with 58 rows.\n",
      "Read data\\H400\\H400_SEC19.csv successfully with 59 rows.\n",
      "Processed all CSV files in H400. Total rows: 1055. Saved to combined_data\\H400_combined.csv\n",
      "Read data\\L100\\L100_SEC01.csv successfully with 59 rows.\n",
      "Read data\\L100\\L100_SEC02.csv successfully with 61 rows.\n",
      "Read data\\L100\\L100_SEC04.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC05.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC06.csv successfully with 61 rows.\n",
      "Read data\\L100\\L100_SEC07.csv successfully with 59 rows.\n",
      "Read data\\L100\\L100_SEC08.csv successfully with 61 rows.\n",
      "Read data\\L100\\L100_SEC09.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC10.csv successfully with 59 rows.\n",
      "Read data\\L100\\L100_SEC11.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC12.csv successfully with 59 rows.\n",
      "Read data\\L100\\L100_SEC13.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC14.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC15.csv successfully with 61 rows.\n",
      "Read data\\L100\\L100_SEC16.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC17.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC18.csv successfully with 60 rows.\n",
      "Read data\\L100\\L100_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in L100. Total rows: 1080. Saved to combined_data\\L100_combined.csv\n",
      "Read data\\L400\\L400_SEC01.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC02.csv successfully with 56 rows.\n",
      "Read data\\L400\\L400_SEC04.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC05.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC06.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC07.csv successfully with 58 rows.\n",
      "Read data\\L400\\L400_SEC08.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC09.csv successfully with 57 rows.\n",
      "Read data\\L400\\L400_SEC10.csv successfully with 58 rows.\n",
      "Read data\\L400\\L400_SEC11.csv successfully with 60 rows.\n",
      "Read data\\L400\\L400_SEC12.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC13.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC14.csv successfully with 57 rows.\n",
      "Read data\\L400\\L400_SEC15.csv successfully with 60 rows.\n",
      "Read data\\L400\\L400_SEC16.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC17.csv successfully with 59 rows.\n",
      "Read data\\L400\\L400_SEC18.csv successfully with 58 rows.\n",
      "Read data\\L400\\L400_SEC19.csv successfully with 59 rows.\n",
      "Processed all CSV files in L400. Total rows: 1054. Saved to combined_data\\L400_combined.csv\n",
      "Read data\\M000\\M000_SEC01.csv successfully with 59 rows.\n",
      "Read data\\M000\\M000_SEC02.csv successfully with 56 rows.\n",
      "Read data\\M000\\M000_SEC04.csv successfully with 59 rows.\n",
      "Read data\\M000\\M000_SEC05.csv successfully with 59 rows.\n",
      "Read data\\M000\\M000_SEC06.csv successfully with 59 rows.\n",
      "Read data\\M000\\M000_SEC07.csv successfully with 58 rows.\n",
      "Read data\\M000\\M000_SEC08.csv successfully with 59 rows.\n",
      "Read data\\M000\\M000_SEC09.csv successfully with 57 rows.\n",
      "Read data\\M000\\M000_SEC10.csv successfully with 58 rows.\n",
      "Read data\\M000\\M000_SEC11.csv successfully with 60 rows.\n",
      "Read data\\M000\\M000_SEC13.csv successfully with 59 rows.\n",
      "Read data\\M000\\M000_SEC14.csv successfully with 57 rows.\n",
      "Read data\\M000\\M000_SEC15.csv successfully with 60 rows.\n",
      "Read data\\M000\\M000_SEC16.csv successfully with 59 rows.\n",
      "Read data\\M000\\M000_SEC17.csv successfully with 59 rows.\n",
      "Read data\\M000\\M000_SEC18.csv successfully with 58 rows.\n",
      "Read data\\M000\\M000_SEC19.csv successfully with 59 rows.\n",
      "Processed all CSV files in M000. Total rows: 995. Saved to combined_data\\M000_combined.csv\n",
      "Read data\\M100\\M100_SEC01.csv successfully with 59 rows.\n",
      "Read data\\M100\\M100_SEC02.csv successfully with 56 rows.\n",
      "Read data\\M100\\M100_SEC04.csv successfully with 59 rows.\n",
      "Read data\\M100\\M100_SEC05.csv successfully with 59 rows.\n",
      "Read data\\M100\\M100_SEC06.csv successfully with 59 rows.\n",
      "Read data\\M100\\M100_SEC07.csv successfully with 58 rows.\n",
      "Read data\\M100\\M100_SEC08.csv successfully with 60 rows.\n",
      "Read data\\M100\\M100_SEC09.csv successfully with 58 rows.\n",
      "Read data\\M100\\M100_SEC10.csv successfully with 58 rows.\n",
      "Read data\\M100\\M100_SEC11.csv successfully with 60 rows.\n",
      "Read data\\M100\\M100_SEC12.csv successfully with 59 rows.\n",
      "Read data\\M100\\M100_SEC13.csv successfully with 59 rows.\n",
      "Read data\\M100\\M100_SEC14.csv successfully with 57 rows.\n",
      "Read data\\M100\\M100_SEC15.csv successfully with 60 rows.\n",
      "Read data\\M100\\M100_SEC16.csv successfully with 59 rows.\n",
      "Read data\\M100\\M100_SEC17.csv successfully with 59 rows.\n",
      "Read data\\M100\\M100_SEC18.csv successfully with 58 rows.\n",
      "Read data\\M100\\M100_SEC19.csv successfully with 59 rows.\n",
      "Processed all CSV files in M100. Total rows: 1056. Saved to combined_data\\M100_combined.csv\n",
      "Read data\\M200\\M200_SEC01.csv successfully with 59 rows.\n",
      "Read data\\M200\\M200_SEC013.csv successfully with 59 rows.\n",
      "Read data\\M200\\M200_SEC02.csv successfully with 56 rows.\n",
      "Read data\\M200\\M200_SEC04.csv successfully with 59 rows.\n",
      "Read data\\M200\\M200_SEC05.csv successfully with 60 rows.\n",
      "Read data\\M200\\M200_SEC06.csv successfully with 59 rows.\n",
      "Read data\\M200\\M200_SEC07.csv successfully with 58 rows.\n",
      "Read data\\M200\\M200_SEC08.csv successfully with 60 rows.\n",
      "Read data\\M200\\M200_SEC09.csv successfully with 58 rows.\n",
      "Read data\\M200\\M200_SEC10.csv successfully with 58 rows.\n",
      "Read data\\M200\\M200_SEC11.csv successfully with 60 rows.\n",
      "Read data\\M200\\M200_SEC12.csv successfully with 59 rows.\n",
      "Read data\\M200\\M200_SEC14.csv successfully with 57 rows.\n",
      "Read data\\M200\\M200_SEC15.csv successfully with 61 rows.\n",
      "Read data\\M200\\M200_SEC16.csv successfully with 59 rows.\n",
      "Read data\\M200\\M200_SEC17.csv successfully with 59 rows.\n",
      "Read data\\M200\\M200_SEC18.csv successfully with 58 rows.\n",
      "Read data\\M200\\M200_SEC19.csv successfully with 59 rows.\n",
      "Processed all CSV files in M200. Total rows: 1058. Saved to combined_data\\M200_combined.csv\n",
      "Read data\\M300\\M300_SEC01.csv successfully with 59 rows.\n",
      "Read data\\M300\\M300_SEC02.csv successfully with 56 rows.\n",
      "Read data\\M300\\M300_SEC04.csv successfully with 59 rows.\n",
      "Read data\\M300\\M300_SEC05.csv successfully with 60 rows.\n",
      "Read data\\M300\\M300_SEC06.csv successfully with 59 rows.\n",
      "Read data\\M300\\M300_SEC07.csv successfully with 58 rows.\n",
      "Read data\\M300\\M300_SEC08.csv successfully with 60 rows.\n",
      "Read data\\M300\\M300_SEC09.csv successfully with 58 rows.\n",
      "Read data\\M300\\M300_SEC10.csv successfully with 58 rows.\n",
      "Read data\\M300\\M300_SEC11.csv successfully with 60 rows.\n",
      "Read data\\M300\\M300_SEC12.csv successfully with 59 rows.\n",
      "Read data\\M300\\M300_SEC13.csv successfully with 59 rows.\n",
      "Read data\\M300\\M300_SEC14.csv successfully with 57 rows.\n",
      "Read data\\M300\\M300_SEC15.csv successfully with 60 rows.\n",
      "Read data\\M300\\M300_SEC16.csv successfully with 59 rows.\n",
      "Read data\\M300\\M300_SEC17.csv successfully with 59 rows.\n",
      "Read data\\M300\\M300_SEC18.csv successfully with 58 rows.\n",
      "Read data\\M300\\M300_SEC19.csv successfully with 59 rows.\n",
      "Processed all CSV files in M300. Total rows: 1057. Saved to combined_data\\M300_combined.csv\n",
      "Read data\\M400\\M400_SEC01.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC02.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC04.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC05.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC06.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC07.csv successfully with 58 rows.\n",
      "Read data\\M400\\M400_SEC08.csv successfully with 62 rows.\n",
      "Read data\\M400\\M400_SEC09.csv successfully with 58 rows.\n",
      "Read data\\M400\\M400_SEC10.csv successfully with 59 rows.\n",
      "Read data\\M400\\M400_SEC11.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC12.csv successfully with 59 rows.\n",
      "Read data\\M400\\M400_SEC13.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC14.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC15.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC16.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC17.csv successfully with 60 rows.\n",
      "Read data\\M400\\M400_SEC18.csv successfully with 59 rows.\n",
      "Read data\\M400\\M400_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in M400. Total rows: 1075. Saved to combined_data\\M400_combined.csv\n",
      "Read data\\S100\\S100_SEC01.csv successfully with 59 rows.\n",
      "Read data\\S100\\S100_SEC02.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC04.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC05.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC06.csv successfully with 61 rows.\n",
      "Read data\\S100\\S100_SEC07.csv successfully with 59 rows.\n",
      "Read data\\S100\\S100_SEC08.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC09.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC10.csv successfully with 59 rows.\n",
      "Read data\\S100\\S100_SEC11.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC12.csv successfully with 59 rows.\n",
      "Read data\\S100\\S100_SEC13.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC14.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC15.csv successfully with 61 rows.\n",
      "Read data\\S100\\S100_SEC16.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC17.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC18.csv successfully with 60 rows.\n",
      "Read data\\S100\\S100_SEC19.csv successfully with 60 rows.\n",
      "Processed all CSV files in S100. Total rows: 1078. Saved to combined_data\\S100_combined.csv\n",
      "Read data\\X100\\X102.csv successfully with 1062 rows.\n",
      "Processed all CSV files in X100. Total rows: 1062. Saved to combined_data\\X100_combined.csv\n",
      "Finished processing all subdirectories.\n"
     ]
    }
   ],
   "source": [
    "def process_csv_files(base_path, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    subdirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        # Get all CSV files in the current subdirectory\n",
    "        csv_files = glob(os.path.join(base_path, subdir, '*.csv'))\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in {os.path.join(base_path, subdir)}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize an empty list to store DataFrames for this subdirectory\n",
    "        subdir_dataframes = []\n",
    "\n",
    "        # Read the first CSV file to get the correct column headers\n",
    "        first_file = csv_files[0]\n",
    "        initial_df = pd.read_csv(first_file)\n",
    "        correct_columns = initial_df.columns\n",
    "        subdir_dataframes.append(initial_df)\n",
    "        print(f\"Read {first_file} successfully with {len(initial_df)} rows.\")\n",
    "\n",
    "        # Process the rest of the files\n",
    "        for file in csv_files[1:]:\n",
    "            try:\n",
    "                df = pd.read_csv(file, header=None, names=correct_columns) # Read the file with the correct columns\n",
    "                df.columns = correct_columns\n",
    "                \n",
    "                # Check if the columns match\n",
    "                if list(df.columns) != list(correct_columns):\n",
    "                    print(f\"Warning: Columns in {file} do not match the expected columns. Skipping this file.\")\n",
    "                    continue\n",
    "                \n",
    "                subdir_dataframes.append(df)\n",
    "                print(f\"Read {file} successfully with {len(df)} rows.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {str(e)}\")\n",
    "\n",
    "        # Combine all DataFrames for this subdirectory\n",
    "        combined_df = pd.concat(subdir_dataframes, ignore_index=True)\n",
    "        \n",
    "        # Save the combined DataFrame for this subdirectory directly in the output_path\n",
    "        output_file = os.path.join(output_path, f\"{subdir}_combined.csv\")\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed all CSV files in {subdir}. Total rows: {len(combined_df)}. Saved to {output_file}\")\n",
    "\n",
    "    print(\"Finished processing all subdirectories.\")\n",
    "\n",
    "# Usage\n",
    "base_path = 'data'\n",
    "output_path = 'combined_data'\n",
    "process_csv_files(base_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: combined_data\\C100_combined.csv\n",
      "Warning: Column 'availability' not found in combined_data\\C100_combined.csv. Skipping removal.\n",
      "Warning: Column 'username' not found in combined_data\\C100_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_C100_combined.csv\n",
      "Processing file: combined_data\\C200_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\C200_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_C200_combined.csv\n",
      "Processing file: combined_data\\C300_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\C300_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_C300_combined.csv\n",
      "Processing file: combined_data\\C400_combined.csv\n",
      "Warning: Column 'availability' not found in combined_data\\C400_combined.csv. Skipping removal.\n",
      "Warning: Column 'Username' not found in combined_data\\C400_combined.csv. Skipping removal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and processed data saved to: cleaned_data\\cleaned_C400_combined.csv\n",
      "Processing file: combined_data\\C500_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\C500_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_C500_combined.csv\n",
      "Processing file: combined_data\\F100_combined.csv\n",
      "Warning: Column 'availability' not found in combined_data\\F100_combined.csv. Skipping removal.\n",
      "Warning: Column 'Username' not found in combined_data\\F100_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_F100_combined.csv\n",
      "Processing file: combined_data\\H100_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\H100_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_H100_combined.csv\n",
      "Processing file: combined_data\\H400_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Column 'Username' not found in combined_data\\H400_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_H400_combined.csv\n",
      "Processing file: combined_data\\L100_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\L100_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_L100_combined.csv\n",
      "Processing file: combined_data\\L400_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\L400_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_L400_combined.csv\n",
      "Processing file: combined_data\\M000_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\M000_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_M000_combined.csv\n",
      "Processing file: combined_data\\M100_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Column 'Username' not found in combined_data\\M100_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_M100_combined.csv\n",
      "Processing file: combined_data\\M200_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\M200_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_M200_combined.csv\n",
      "Processing file: combined_data\\M300_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\M300_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_M300_combined.csv\n",
      "Processing file: combined_data\\M400_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\M400_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_M400_combined.csv\n",
      "Processing file: combined_data\\S100_combined.csv\n",
      "Warning: Column 'Username' not found in combined_data\\S100_combined.csv. Skipping removal.\n",
      "Cleaned and processed data saved to: cleaned_data\\cleaned_S100_combined.csv\n",
      "Processing file: combined_data\\X100_combined.csv\n",
      "Error: 'learner_id' column not found in combined_data\\X100_combined.csv.\n",
      "Available columns: ['Last Name', 'First Name', 'Learner ID', 'Username', 'Last Access', 'Availability', 'X102 Board Learner Acknowledgement of Recording Privacy Act Satement [Total Pts: 100 Complete/Incomplete] |402665', 'X100A Oral Board [Total Pts: 100 Score] |402662', 'X100B Online Exam [Total Pts: 100 Score] |402663', 'X100 Weighted Total [Total Pts: up to 100 Percentage] |402664', 'Average X100 Oral Board [Total Pts: up to 100 Percentage] |402666', '#1Chairperson [Total Pts: 100 Score] |402667', '#2 Instructor [Total Pts: 100 Score] |402669', '#3 Instructor [Total Pts: 100 Score] |402670', 'Ave of PreTest [Total Pts: up to 100 Score] |402671', 'Weighted Total [Total Pts: up to 0 Percentage] |402672', 'AY22_X100_Pretest (IMS) [Total Pts: 100 Score] |402674', 'Re-Test - #1 Chairperson [Total Pts: 100 Score] |402678', 'Re-Test - #2 Instructor [Total Pts: 100 Score] |402679', 'Re-Test- #3 Instructor [Total Pts: 100 Score] |402680', 'AY24_X102_Online Exam (US) [Total Pts: 100 Score] |422559', 'AY24_X102_Online Exam (IMS) [Total Pts: 100 Score] |422560']\n",
      "Finished processing all files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_18916\\2769829327.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n"
     ]
    }
   ],
   "source": [
    "def load_grading_scale(file_path):\n",
    "    grading_df = pd.read_csv(file_path)\n",
    "    grade_map = dict(zip(grading_df['Letter Grade'], grading_df['4-Point Equivalence']))\n",
    "    return grade_map\n",
    "\n",
    "def convert_grade(value, grade_map):\n",
    "    if isinstance(value, str):\n",
    "        return grade_map.get(value.upper(), value)\n",
    "    return value\n",
    "\n",
    "def clean_and_process_data(input_path, output_path, grading_scale_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    grade_map = load_grading_scale(grading_scale_path)\n",
    "    \n",
    "    csv_files = glob(os.path.join(input_path, '*_combined.csv')) # Using glob function directly\n",
    "\n",
    "    for file in csv_files:\n",
    "        print(f\"Processing file: {file}\")\n",
    "        \n",
    "        df = pd.read_csv(file)\n",
    "        df.dropna(how='all', inplace=True)\n",
    "\n",
    "        if 'learner_id' not in df.columns:\n",
    "            print(f\"Error: 'learner_id' column not found in {file}.\")\n",
    "            print(\"Available columns:\", df.columns.tolist())\n",
    "            continue\n",
    "\n",
    "        df['team'] = pd.to_numeric(df['learner_id'].str[:2], errors='coerce').astype('Int64')\n",
    "        df['section'] = df['learner_id'].str[-1]\n",
    "\n",
    "        if 'last_name' in df.columns and 'first_name' in df.columns:\n",
    "            df['base_id'] = (df['last_name'].str[0].fillna('') + \n",
    "                             df['first_name'].str[0].fillna('') + \n",
    "                             df['learner_id'].fillna(''))\n",
    "        else:\n",
    "            print(f\"Warning: 'last_name' or 'first_name' columns not found in {file}. Using only 'learner_id' for base_id.\")\n",
    "            df['base_id'] = df['learner_id'].fillna('')\n",
    "\n",
    "        def create_unique_id(group):\n",
    "            if len(group) == 1:\n",
    "                return group['base_id']\n",
    "            else:\n",
    "                return group['base_id'] + '_' + (group.groupby('base_id').cumcount() + 1).astype(str)\n",
    "\n",
    "        df['unique_id'] = df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
    "\n",
    "        columns_to_remove = ['availability', 'username', 'Username', 'learner_id', 'base_id']\n",
    "        for col in columns_to_remove:\n",
    "            if col in df.columns:\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in {file}. Skipping removal.\")\n",
    "\n",
    "        # Round numeric columns to 2 decimal places\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].round(2)\n",
    "\n",
    "        # Convert 'pass' to 1 and 'fail' to 0\n",
    "        df = df.replace({'Pass': 1, 'Fail': 0})\n",
    "\n",
    "        # Apply grading scale conversion\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].apply(lambda x: convert_grade(x, grade_map))\n",
    "\n",
    "        columns = df.columns.tolist()\n",
    "        columns.remove('unique_id')\n",
    "        columns = ['unique_id'] + columns\n",
    "        df = df[columns]\n",
    "\n",
    "        output_file = os.path.join(output_path, f\"cleaned_{os.path.basename(file)}\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Cleaned and processed data saved to: {output_file}\")\n",
    "\n",
    "    print(\"Finished processing all files.\")\n",
    "\n",
    "# Usage\n",
    "input_path = 'combined_data'\n",
    "output_path = 'cleaned_data'\n",
    "grading_scale_path = 'grading_scale.csv'\n",
    "clean_and_process_data(input_path, output_path, grading_scale_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

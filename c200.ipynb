{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data/C200/c200_SEC01.csv successfully with 59 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    'data/C200/c200_SEC01.csv', 'data/C200/c200_SEC02.csv', 'data/C200/c200_SEC04.csv',\n",
    "    'data/C200/c200_SEC05.csv', 'data/C200/c200_SEC06.csv', 'data/C200/c200_SEC07.csv',\n",
    "    'data/C200/c200_SEC08.csv', 'data/C200/c200_SEC09.csv', 'data/C200/c200_SEC10.csv',\n",
    "    'data/C200/c200_SEC11.csv', 'data/C200/c200_SEC12.csv', 'data/C200/c200_SEC13.csv',\n",
    "    'data/C200/c200_SEC14.csv', 'data/C200/c200_SEC15.csv', 'data/C200/c200_SEC16.csv',\n",
    "    'data/C200/c200_SEC17.csv', 'data/C200/c200_SEC18.csv', 'data/C200/c200_SEC19.csv'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read the first CSV file to get the correct column headers\n",
    "initial_df = pd.read_csv(csv_files[0])\n",
    "correct_columns = initial_df.columns\n",
    "\n",
    "# Process the first file\n",
    "dataframes.append(initial_df)\n",
    "print(f\"Read {csv_files[0]} successfully with {len(initial_df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data/C200/c200_SEC02.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC04.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC05.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC06.csv successfully with 60 rows.\n",
      "Read data/C200/c200_SEC07.csv successfully with 58 rows.\n",
      "Read data/C200/c200_SEC08.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC09.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC10.csv successfully with 58 rows.\n",
      "Read data/C200/c200_SEC11.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC12.csv successfully with 58 rows.\n",
      "Read data/C200/c200_SEC13.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC14.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC15.csv successfully with 60 rows.\n",
      "Read data/C200/c200_SEC16.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC17.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC18.csv successfully with 59 rows.\n",
      "Read data/C200/c200_SEC19.csv successfully with 59 rows.\n"
     ]
    }
   ],
   "source": [
    "# Loop through the remaining files\n",
    "for file in csv_files[1:]:\n",
    "    try:\n",
    "        # Read the current CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Rename columns to match the correct columns\n",
    "        df.columns = correct_columns\n",
    "        \n",
    "        print(f\"Read {file} successfully with {len(df)} rows.\")\n",
    "        \n",
    "        # Drop rows where all cells are blank\n",
    "        df.dropna(how='all', inplace=True)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data compiled and saved successfully.\n",
      "Total rows in compiled dataframe: 1061\n",
      "\n",
      "Summary of rows per section:\n",
      "section\n",
      "A    281\n",
      "B    259\n",
      "C    264\n",
      "D    257\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data with reordered columns:\n",
      "  unique_id     last_name                first_name               username  \\\n",
      "0     AM01C        ACOSTA                  MITCHELL         mitch.f.acosta   \n",
      "1     AJ01A         ALLEN                    JUSTIN       justin.lee.allen   \n",
      "2     AK01A       ALLISON                     KEVIN        kevin.e.allison   \n",
      "3     AA01D     ALSUWAIDI  AHMED SAIF ABDULLA AHMED   alsuwaidi@hotmail.fr   \n",
      "4     AA01C      ANDERSON                      ALEX        alex.r.anderson   \n",
      "5     AA01B      ANDERSON                   ANTHONY    anthony.s.anderson2   \n",
      "6     AR01D      ANDERSON                    ROBERT  robert.edwin.anderson   \n",
      "7   AS01C_1      ANDERSON                    STACEY      stacey.c.anderson   \n",
      "8     AS01B      ANDERSON                   STEPHEN      stephen.anderson8   \n",
      "9     AJ01C  ANDRADE LAZO              JOSE ARMANDO    joseaal@hotmail.com   \n",
      "\n",
      "  learner_id       last_access availability  c200_wt_ttl   c200_ttl  \\\n",
      "0        01C     2/9/2024 9:33          Yes     95.38322  374.34050   \n",
      "1        01A   1/20/2024 12:04          Yes     93.45000  375.00000   \n",
      "2        01A  11/12/2023 16:31          Yes     92.15000  370.00000   \n",
      "3        01D   2/12/2024 10:01          Yes     92.01205  366.29008   \n",
      "4        01C    2/28/2024 7:46          Yes     94.30665  375.58875   \n",
      "5        01B    2/26/2024 9:48          Yes     94.30000  383.00000   \n",
      "6        01D   2/26/2024 18:12          Yes     83.64869  339.99564   \n",
      "7        01C   2/15/2024 13:49          Yes     94.33679  376.99358   \n",
      "8        01B  11/13/2023 11:11          Yes     90.50000  364.00000   \n",
      "9        01C    2/13/2024 9:34          Yes     88.30707  364.69100   \n",
      "\n",
      "   c200_ctgl_299  c200_ctgl_200 c200_essay  c200_test_us  c200_test_ims  team  \\\n",
      "0        92.3940       90.99750     98.949          92.0            NaN     1   \n",
      "1        93.0000       90.00000       92.0         100.0            NaN     1   \n",
      "2        93.0000       90.00000       91.0          96.0            NaN     1   \n",
      "3        87.0000       91.09425   92.19583           NaN           96.0     1   \n",
      "4        90.9975       90.39600   94.19525         100.0            NaN     1   \n",
      "5        96.0000       96.00000       91.0         100.0            NaN     1   \n",
      "6        94.0000       90.99750   82.99814          72.0            NaN     1   \n",
      "7        97.5000       96.10000   95.39358          88.0            NaN     1   \n",
      "8        92.0000       94.00000       90.0          88.0            NaN     1   \n",
      "9        92.9975       93.39800    82.2955           NaN           96.0     1   \n",
      "\n",
      "  section base_id  \n",
      "0       C   AM01C  \n",
      "1       A   AJ01A  \n",
      "2       A   AK01A  \n",
      "3       D   AA01D  \n",
      "4       C   AA01C  \n",
      "5       B   AA01B  \n",
      "6       D   AR01D  \n",
      "7       C   AS01C  \n",
      "8       B   AS01B  \n",
      "9       C   AJ01C  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_19024\\2208565859.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  c200_df['unique_id'] = c200_df.groupby('base_id', group_keys=False).apply(create_unique_id)\n"
     ]
    }
   ],
   "source": [
    "if dataframes:\n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    c200_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Process the learner_id column\n",
    "    c200_df['team'] = c200_df['learner_id'].str[:2].astype(int)\n",
    "    c200_df['section'] = c200_df['learner_id'].str[-1]\n",
    "    \n",
    "    # Create a base for the unique identifier\n",
    "    c200_df['base_id'] = (c200_df['last_name'].str[0] + \n",
    "                          c200_df['first_name'].str[0] + \n",
    "                          c200_df['learner_id'])\n",
    "    \n",
    "    # Function to create a truly unique identifier\n",
    "    def create_unique_id(group):\n",
    "        if len(group) == 1:\n",
    "            return group['base_id']\n",
    "        else:\n",
    "            return group['base_id'] + '_' + (group.groupby('base_id').cumcount() + 1).astype(str)\n",
    "    \n",
    "    # Apply the function to create unique identifiers\n",
    "    c200_df['unique_id'] = c200_df.groupby('base_id', group_keys=False).apply(create_unique_id)\n",
    "    \n",
    "    # Reorder columns to move unique_id to the leftmost position\n",
    "    columns = c200_df.columns.tolist()\n",
    "    columns.remove('unique_id')\n",
    "    columns = ['unique_id'] + columns\n",
    "    c200_df = c200_df[columns]\n",
    "    \n",
    "    # Save the compiled DataFrame\n",
    "    c200_df.to_csv('cleaned_data/c200_compiled_dataframe.csv', index=False)\n",
    "    print(\"Data compiled and saved successfully.\")\n",
    "    print(f\"Total rows in compiled dataframe: {len(c200_df)}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary of rows per section:\")\n",
    "    print(c200_df['section'].value_counts().sort_index())\n",
    "    \n",
    "    # Display the first few rows to verify the new column order\n",
    "    print(\"\\nSample data with reordered columns:\")\n",
    "    print(c200_df.head(10))\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

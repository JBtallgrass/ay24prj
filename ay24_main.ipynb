{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score \n",
    "import statsmodels.api as sm\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite3 version: 3.45.3\n",
      "SQLAlchemy engine created: Engine(sqlite:///test.db)\n"
     ]
    }
   ],
   "source": [
    "# Verify sqlite3\n",
    "print(\"sqlite3 version:\", sqlite3.sqlite_version)\n",
    "\n",
    "# Verify SQLAlchemy\n",
    "engine = create_engine('sqlite:///test.db')\n",
    "print(\"SQLAlchemy engine created:\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the List of CSV Files and Set Up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data folder path\n",
    "data_folder = 'data'\n",
    "\n",
    "# List all CSV files in the data folder\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('cleaned_data', exist_ok=True)\n",
    "os.makedirs('text_data', exist_ok=True)\n",
    "os.makedirs('images', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extract and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved to: cleaned_data\\C100.csv\n",
      "Cleaned file saved to: cleaned_data\\C200.csv\n",
      "Cleaned file saved to: cleaned_data\\C300.csv\n",
      "Cleaned file saved to: cleaned_data\\C400.csv\n",
      "Cleaned file saved to: cleaned_data\\C500.csv\n",
      "Cleaned file saved to: cleaned_data\\F100.csv\n",
      "Cleaned file saved to: cleaned_data\\H100.csv\n",
      "Cleaned file saved to: cleaned_data\\L100.csv\n",
      "Cleaned file saved to: cleaned_data\\M000.csv\n",
      "Cleaned file saved to: cleaned_data\\M100.csv\n",
      "Cleaned file saved to: cleaned_data\\M200.csv\n",
      "Cleaned file saved to: cleaned_data\\M300.csv\n",
      "Cleaned file saved to: cleaned_data\\M400.csv\n",
      "Cleaned file saved to: cleaned_data\\S100.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_19940\\4101726831.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result = df.replace({\"Needs Evaluation\": np.nan, \"Fail\": 0, \"Pass\": 1})\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_19940\\4101726831.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result = df.replace({\"Needs Evaluation\": np.nan, \"Fail\": 0, \"Pass\": 1})\n"
     ]
    }
   ],
   "source": [
    "# Set future behavior for downcasting in replace\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# Function to clean and save a single CSV file\n",
    "def clean_and_save_csv(file_path):\n",
    "    # Read the CSV file with low_memory to handle mixed data types\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "    # Replace blank cells with NaN, \"Needs Evaluation\" with NaN, \"Fail\" with 0, and \"Pass\" with 1\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "    # Apply the old behavior for downcasting in replace\n",
    "    result = df.replace({\"Needs Evaluation\": np.nan, \"Fail\": 0, \"Pass\": 1})\n",
    "    result.infer_objects(copy=False)\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "    \n",
    "    # Define cleaned file path\n",
    "    cleaned_file_path = os.path.join('cleaned_data', os.path.basename(file_path))\n",
    "    \n",
    "    # Save cleaned DataFrame\n",
    "    df.to_csv(cleaned_file_path, index=False)\n",
    "    print(f\"Cleaned file saved to: {cleaned_file_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each CSV file and store the cleaned DataFrames\n",
    "cleaned_dataframes = [clean_and_save_csv(os.path.join(data_folder, file)) for file in csv_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all cleaned CSV files into a list of DataFrames with block information\n",
    "data_frames = []\n",
    "for file in csv_files:\n",
    "    block_name = file.split('.')[0]  # Extract block name from filename (before the extension)\n",
    "    df = pd.read_csv(os.path.join('cleaned_data', file))\n",
    "    \n",
    "    # Create the unique identifier\n",
    "    df['unique_id'] = df['last_name'].str[0] + df['first_name'].str[0] + df['learnerID'].astype(str)\n",
    "    df['block'] = block_name  # Add block name as a new column\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Consolidate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Handle missing values and data types as necessary\n",
    "combined_df.fillna(0, inplace=True)  # Example of handling missing values\n",
    "combined_df['team'] = combined_df['team'].astype(int)  # Ensure 'team' column is integer\n",
    "\n",
    "# Create helper columns if needed\n",
    "combined_df['team_section'] = combined_df['team'].astype(str) + combined_df['section']\n",
    "\n",
    "# Automatically identify numeric columns for analysis\n",
    "numeric_columns = combined_df.select_dtypes(include=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load Data into a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection string (example using SQLite)\n",
    "db_connection_string = 'sqlite:///data/transformed_data.db'\n",
    "engine = create_engine(db_connection_string)\n",
    "\n",
    "# Load data into the database\n",
    "combined_df.to_sql('consolidated_table', engine, index=False, if_exists='replace')\n",
    "\n",
    "print(\"Data loaded successfully into the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct Analysis Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection string (example using SQLite)\n",
    "db_connection_string = 'sqlite:///data/transformed_data.db'\n",
    "engine = create_engine(db_connection_string)\n",
    "\n",
    "# Load data into the database\n",
    "combined_df.to_sql('consolidated_table', engine, index=False, if_exists='replace')\n",
    "\n",
    "print(\"Data loaded successfully into the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SQL Queries Dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query(group_by_col, numeric_cols):\n",
    "    # Generate the SQL query dynamically based on the numeric columns\n",
    "    select_statements = [f\"AVG({col}) as avg_{col}, MEDIAN({col}) as median_{col}, STDDEV({col}) as std_{col}, \" \\\n",
    "                         f\"COUNT({col}) as count_{col}, MIN({col}) as min_{col}, MAX({col}) as max_{col}\" for col in numeric_cols]\n",
    "    query = f\"\"\"\n",
    "    SELECT {group_by_col},\n",
    "           {', '.join(select_statements)}\n",
    "    FROM consolidated_table\n",
    "    GROUP BY {group_by_col};\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('data/transformed_data.db')\n",
    "\n",
    "# Define functions for analysis\n",
    "def analyze_by_team(numeric_cols):\n",
    "    query = generate_sql_query('team', numeric_cols)\n",
    "    return pd.read_sql(query, conn)\n",
    "\n",
    "def analyze_by_section(numeric_cols):\n",
    "    query = generate_sql_query('team_section', numeric_cols)\n",
    "    return pd.read_sql(query, conn)\n",
    "\n",
    "def analyze_by_individual(numeric_cols):\n",
    "    query = generate_sql_query('unique_id, block', numeric_cols)\n",
    "    return pd.read_sql(query, conn)\n",
    "\n",
    "def correlation_analysis():\n",
    "    query = \"SELECT * FROM consolidated_table\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    return df.corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Analysis and Generate Visualizations Dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct analysis for each block\n",
    "blocks = combined_df['block'].unique()  # Get unique blocks\n",
    "analysis_results = {}\n",
    "for block in blocks:\n",
    "    block_df = combined_df[combined_df['block'] == block]\n",
    "    team_performance = analyze_by_team(numeric_columns)\n",
    "    section_performance = analyze_by_section(numeric_columns)\n",
    "    individual_performance = analyze_by_individual(numeric_columns)\n",
    "    correlation_matrix = correlation_analysis()\n",
    "    \n",
    "    # Store results in a dictionary for easy access later\n",
    "    analysis_results[block] = {\n",
    "        'team_performance': team_performance,\n",
    "        'section_performance': section_performance,\n",
    "        'individual_performance': individual_performance,\n",
    "        'correlation_matrix': correlation_matrix\n",
    "    }\n",
    "    \n",
    "    # Write the analysis results to a text file\n",
    "    with open(f'text_data/{block}_analysis.txt', 'w') as file:\n",
    "        file.write(f'Analysis for {block}\\n')\n",
    "        file.write('Team Performance:\\n')\n",
    "        file.write(team_performance.to_string())\n",
    "        file.write('\\n\\nSection Performance:\\n')\n",
    "        file.write(section_performance.to_string())\n",
    "        file.write('\\n\\nIndividual Performance:\\n')\n",
    "        file.write(individual_performance.to_string())\n",
    "        file.write('\\n\\nCorrelation Matrix:\\n')\n",
    "        file.write(correlation_matrix.to_string())\n",
    "    \n",
    "    # Create and save visualizations for team performance\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=team_performance['team'], y=team_performance[f'avg_{col}'] )\n",
    "        plt.title(f'Team Performance in {block.capitalize()} ({col})')\n",
    "        plt.savefig(f'images/{block}_team_performance_{col}.png')\n",
    "        plt.close()\n",
    "\n",
    "    # Create and save visualizations for section performance\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=section_performance['team_section'], y=section_performance[f'avg_{col}'])\n",
    "        plt.title(f'Section Performance in {block.capitalize()} ({col})')\n",
    "        plt.savefig(f'images/{block}_section_performance_{col}.png')\n",
    "        plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

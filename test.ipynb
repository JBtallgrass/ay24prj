{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score \n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "import hashlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the main 'data' directory\n",
    "data_directory = \"data\"\n",
    "\n",
    "# Define the absolute path to the column mapping CSV file\n",
    "column_mapping_file = \"path/to/column_mapping.csv\"  # Update this to the correct path\n",
    "\n",
    "# List of block directories\n",
    "block_directories = [\n",
    "    \"C100\", \"C200\", \"C300\", \"C400\", \"C500\",\n",
    "    \"L100\", \"L400\", \"H100\", \"H400\",\n",
    "    \"M000\", \"M100\", \"M200\", \"M300\", \"M400\",\n",
    "    \"S100\", \"F100\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the column mapping CSV into a DataFrame\n",
    "column_mapping_df = pd.read_csv(column_mapping_file)\n",
    "\n",
    "# Create a dictionary from the DataFrame\n",
    "column_mapping = dict(zip(column_mapping_df['original'], column_mapping_df['new']))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to anonymize names\n",
    "def anonymize_name(row):\n",
    "    full_name = row['First Name'] + row['Last Name']\n",
    "    return hashlib.sha256(full_name.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each block directory and read CSV files\n",
    "for block in block_directories:\n",
    "    block_path = os.path.join(data_directory, block)\n",
    "    \n",
    "    # Check if the block directory exists\n",
    "    if os.path.exists(block_path) and os.path.isdir(block_path):\n",
    "        for file_name in os.listdir(block_path):\n",
    "            if file_name.endswith(\".csv\"):\n",
    "                file_path = os.path.join(block_path, file_name)\n",
    "                print(f\"Reading file: {file_path}\")\n",
    "                \n",
    "                try:\n",
    "                    # Read the CSV file into a DataFrame\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Append the DataFrame to the list\n",
    "                    dfs.append(df)\n",
    "                    \n",
    "                    # Print the shape and columns of the DataFrame\n",
    "                    print(f\"Shape of the DataFrame: {df.shape}\")\n",
    "                    print(f\"Columns in the DataFrame: {df.columns.tolist()}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape and columns of the combined DataFrame\n",
    "print(f\"Shape of the combined DataFrame: {combined_df.shape}\")\n",
    "print(f\"Columns in the combined DataFrame: {combined_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns based on the mapping\n",
    "combined_df = combined_df.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anonymize names, create unique keys, and drop the original name columns\n",
    "if 'First Name' in combined_df.columns and 'Last Name' in combined_df.columns:\n",
    "    combined_df['Student Key'] = combined_df.apply(anonymize_name, axis=1)\n",
    "    combined_df = combined_df.drop(columns=['First Name', 'Last Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize unique_usernames as None\n",
    "unique_usernames = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique IDs to each username and drop the username column\n",
    "if 'username' in combined_df.columns:\n",
    "    unique_usernames = combined_df['username'].unique()\n",
    "    username_mapping = {username: idx for idx, username in enumerate(unique_usernames)}\n",
    "    combined_df['User ID'] = combined_df['username'].map(username_mapping)\n",
    "    combined_df = combined_df.drop(columns=['username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'Learner ID' into 'Team Number' and 'Staff Group'\n",
    "if 'Learner ID' in combined_df.columns:\n",
    "    combined_df['Team Number'] = combined_df['Learner ID'].str.extract(r'(\\d+)', expand=False)\n",
    "    combined_df['Staff Group'] = combined_df['Learner ID'].str.extract(r'([A-D])', expand=False)\n",
    "    \n",
    "    # Convert 'Team Number' to numeric and handle NaN values\n",
    "    combined_df['Team Number'] = pd.to_numeric(combined_df['Team Number'], errors='coerce')\n",
    "    \n",
    "    # Optional: Drop rows with NaN values in 'Team Number' or 'Staff Group' or fill NaN values\n",
    "    combined_df = combined_df.dropna(subset=['Team Number', 'Staff Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'Team Number' and 'Staff Group'\n",
    "combined_df = combined_df.sort_values(by=['Team Number', 'Staff Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a new CSV file\n",
    "output_file = os.path.join(data_directory, 'combined_anonymized_data.csv')\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print the path to the CSV file for review\n",
    "print(f\"The combined and anonymized DataFrame has been saved to: {output_file}\")\n",
    "\n",
    "# Print unique usernames\n",
    "if unique_usernames is not None:\n",
    "    print(\"Unique Usernames:\")\n",
    "    for username in unique_usernames:\n",
    "        print(username)\n",
    "    \n",
    "    # Optionally save the unique usernames to a CSV file\n",
    "    usernames_output_file = os.path.join(data_directory, 'unique_usernames.csv')\n",
    "    pd.Series(unique_usernames).to_csv(usernames_output_file, index=False, header=['username'])\n",
    "    print(f\"Unique usernames have been saved to: {usernames_output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
